{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/filip/samsara/elysium/externals/Noresqa\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import librosa\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from model import NORESQA\n",
    "from scipy import signal\n",
    "import glob\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NORESQA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17902/2605193738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNORESQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NORESQA' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:{}\".format(1))\n",
    "model = NORESQA()\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stft(audio, sampling_rate = 16000):\n",
    "\n",
    "    f, t, Sxx = signal.stft(audio, sampling_rate, window='hann',nperseg=512,noverlap=256,nfft=512)\n",
    "    Sxx = Sxx[:256,:]\n",
    "\n",
    "    feat = np.concatenate((np.abs(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1]), np.angle(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1])), axis=2)\n",
    "\n",
    "    return feat \n",
    "\n",
    "# function doing prediction\n",
    "def model_prediction(test_feat, nmr_feat):\n",
    "\n",
    "    intervals_sdr = np.arange(0.5,40,1) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ranking_frame,sdr_frame,snr_frame = model(test_feat.permute(0,3,2,1),nmr_feat.permute(0,3,2,1))\n",
    "        # preference task prediction\n",
    "        \n",
    "        ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "        pout = ranking[0][0]\n",
    "\n",
    "        # quantification task\n",
    "        sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "        qout = sdr.sum()\n",
    "\n",
    "    return pout, qout\n",
    "\n",
    "# function checking if the size of the inputs are same. If not, then the reference audio's size is adjusted\n",
    "def check_size(audio_ref,audio_test):\n",
    "    \n",
    "    if len(audio_ref) > len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "        \n",
    "    elif len(audio_ref) < len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        while len(audio_test) > len(audio_ref):\n",
    "            audio_ref = np.append(audio_ref, audio_ref)\n",
    "        audio_ref = audio_ref[:len(audio_test)] \n",
    "    \n",
    "    return audio_ref, audio_test\n",
    "\n",
    "# reading audio clips\n",
    "def audio_loading(path,sampling_rate=16000):\n",
    "\n",
    "    audio, fs = librosa.load(path)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    if fs != sampling_rate:\n",
    "        audio = librosa.resample(audio,fs,sampling_rate)\n",
    "\n",
    "    return audio\n",
    "\n",
    "# top level function combining loading, and feature extraction\n",
    "def feats_loading(ref_path,test_path):\n",
    "    \n",
    "    audio_ref = audio_loading(ref_path)\n",
    "    audio_test = audio_loading(test_path)\n",
    "\n",
    "    audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "    ref_feat = extract_stft(audio_ref)\n",
    "    test_feat = extract_stft(audio_test)\n",
    "\n",
    "    return ref_feat,test_feat\n",
    "\n",
    "def inference_file(nmr,test_file):\n",
    "    nmr_feat,test_feat = feats_loading(nmr,test_file)\n",
    "    test_feat = torch.from_numpy(test_feat).float().to(device).unsqueeze(0)\n",
    "    nmr_feat = torch.from_numpy(nmr_feat).float().to(device).unsqueeze(0)\n",
    "\n",
    "    return model_prediction(test_feat,nmr_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feats_loading('/work/data/timit/data/TEST/DR1/FAKS0/SI943.WAV',\n",
    "#glob.glob(\"/work/data/speech_metrics_eval/TencentCorups/withReverberationTrainDev/*\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9049/230907824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Loading checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./externals/metrics/Noresqa/models/model_noresqa_mos.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_base'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    775\u001b[0m             \"functionality.\")\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/filip/samsara/elysium/externals/Noresqa\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import librosa as librosa\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from model import NORESQA\n",
    "from scipy import signal\n",
    "import glob\n",
    "import ntpath\n",
    "\n",
    "model = NORESQA()\n",
    "\n",
    "# Loading checkpoint\n",
    "model_checkpoint_path = './externals/metrics/Noresqa/models/model_noresqa_mos.pth'\n",
    "state = torch.load(model_checkpoint_path,map_location=\"cpu\")['state_base']\n",
    "\n",
    "pretrained_dict = {}\n",
    "for k, v in state.items():\n",
    "    if 'module' in k:\n",
    "        pretrained_dict[k.replace('module.','')]=v\n",
    "    else:\n",
    "        pretrained_dict[k]=v\n",
    "model_dict = model.state_dict()\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(pretrained_dict)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:{}\".format(0))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sfmax = nn.Softmax(dim=1)\n",
    "\n",
    "# function extraction stft\n",
    "def extract_stft(audio, sampling_rate = 16000):\n",
    "\n",
    "    f, t, Sxx = signal.stft(audio, sampling_rate, window='hann',nperseg=512,noverlap=256,nfft=512)\n",
    "    Sxx = Sxx[:256,:]\n",
    "\n",
    "    feat = np.concatenate((np.abs(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1]), np.angle(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1])), axis=2)\n",
    "\n",
    "    return feat \n",
    "\n",
    "# function doing prediction\n",
    "def model_prediction(test_feat, nmr_feat):\n",
    "\n",
    "    intervals_sdr = np.arange(0.5,40,1) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ranking_frame,sdr_frame,snr_frame = model(test_feat.permute(0,3,2,1),nmr_feat.permute(0,3,2,1))\n",
    "        # preference task prediction\n",
    "        \n",
    "        ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "        pout = ranking[0][0]\n",
    "\n",
    "        # quantification task\n",
    "        sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "        qout = sdr.sum()\n",
    "\n",
    "    return pout, qout\n",
    "\n",
    "# function checking if the size of the inputs are same. If not, then the reference audio's size is adjusted\n",
    "def check_size(audio_ref,audio_test):\n",
    "    \n",
    "    if len(audio_ref) > len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "        \n",
    "    elif len(audio_ref) < len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        while len(audio_test) > len(audio_ref):\n",
    "            audio_ref = np.append(audio_ref, audio_ref)\n",
    "        audio_ref = audio_ref[:len(audio_test)] \n",
    "    \n",
    "    return audio_ref, audio_test\n",
    "\n",
    "# reading audio clips\n",
    "def audio_loading(path,sampling_rate=16000):\n",
    "\n",
    "    audio, fs = librosa.load(path)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    if fs != sampling_rate:\n",
    "        audio = librosa.resample(audio,fs,sampling_rate)\n",
    "\n",
    "    return audio\n",
    "\n",
    "# top level function combining loading, and feature extraction\n",
    "def feats_loading(ref_path,test_path):\n",
    "    \n",
    "    audio_ref = audio_loading(ref_path)\n",
    "    audio_test = audio_loading(test_path)\n",
    "\n",
    "    audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "    ref_feat = extract_stft(audio_ref)\n",
    "    test_feat = extract_stft(audio_test)\n",
    "\n",
    "    return ref_feat,test_feat\n",
    "\n",
    "def inference_file(nmr,test_file):\n",
    "    nmr_feat,test_feat = feats_loading(nmr,test_file)\n",
    "    test_feat = torch.from_numpy(test_feat).float().to(device).unsqueeze(0)\n",
    "    nmr_feat = torch.from_numpy(nmr_feat).float().to(device).unsqueeze(0)\n",
    "\n",
    "    return model_prediction(test_feat,nmr_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durations dont match. Adjusting duration of reference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9049/1267914694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mranking_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msdr_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnr_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# preference task prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samsara/elysium/externals/Noresqa/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_encoder_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samsara/elysium/externals/Noresqa/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samsara/elysium/externals/Noresqa/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "x1, x2 = feats_loading('/work/data/timit/data/TEST/DR1/FAKS0/SI943.WAV',\n",
    "glob.glob(\"/work/data/speech_metrics_eval/TencentCorups/withReverberationTrainDev/*\")[0])\n",
    "x1 = torch.from_numpy(x1).float().to(device).unsqueeze(0)\n",
    "x2 = torch.from_numpy(x2).float().to(device).unsqueeze(0)\n",
    "\n",
    "intervals_sdr = np.arange(0.5,40,1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    ranking_frame,sdr_frame,snr_frame = model(x1.permute(0,3,2,1),x2.permute(0,3,2,1))\n",
    "    # preference task prediction\n",
    "    \n",
    "    ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "    pout = ranking[0][0]\n",
    "\n",
    "    # quantification task\n",
    "    sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "    qout = sdr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/filip/samsara/elysium/externals/Noresqa\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import librosa as librosa\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from model import NORESQA\n",
    "from scipy import signal\n",
    "\n",
    "CONFIG_PATH = 'externals/metrics/Noresqa/models/wav2vec_small.pt'\n",
    "metric_type = 1\n",
    "load_checkpoint = False\n",
    "GPU_id = 0\n",
    "# Noresqa model\n",
    "model = NORESQA(40,40,1 #,CONFIG_PATH\n",
    ")\n",
    "if load_checkpoint:\n",
    "# Loading checkpoint\n",
    "    if metric_type==0:\n",
    "        model_checkpoint_path = 'externals/metrics/Noresqa/models/model_noresqa.pth'\n",
    "        state = torch.load(model_checkpoint_path,map_location=\"cpu\")['state_base']\n",
    "    elif metric_type == 1:\n",
    "        model_checkpoint_path = 'externals/metrics/Noresqa/models/model_noresqa_mos.pth'\n",
    "        state = torch.load(model_checkpoint_path,map_location=\"cpu\")['state_dict']\n",
    "\n",
    "    pretrained_dict = {}\n",
    "    for k, v in state.items():\n",
    "        if 'module' in k:\n",
    "            pretrained_dict[k.replace('module.','')]=v\n",
    "        else:\n",
    "            pretrained_dict[k]=v\n",
    "    model_dict = model.state_dict()\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict)\n",
    "\n",
    "# change device as needed\n",
    "# device\n",
    "if GPU_id >=0 and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:{}\".format(GPU_id))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sfmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function extraction stft\n",
    "def extract_stft(audio, sampling_rate = 16000):\n",
    "\n",
    "    fx, tx, stft_out = signal.stft(audio, sampling_rate, window='hann',nperseg=512,noverlap=256,nfft=512)\n",
    "    stft_out = stft_out[:256,:]\n",
    "    feat = np.concatenate((np.abs(stft_out).reshape([stft_out.shape[0],stft_out.shape[1],1]), np.angle(stft_out).reshape([stft_out.shape[0],stft_out.shape[1],1])), axis=2)\n",
    "    return feat\n",
    "\n",
    "# noresqa and noresqa-mos prediction calls\n",
    "def model_prediction_noresqa(test_feat, nmr_feat):\n",
    "\n",
    "    intervals_sdr = np.arange(0.5,40,1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ranking_frame,sdr_frame,snr_frame = model(test_feat.permute(0,3,2,1),nmr_feat.permute(0,3,2,1))\n",
    "        # preference task prediction\n",
    "        ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "        pout = ranking[0][0]\n",
    "        # quantification task\n",
    "        sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "        qout = sdr.sum()\n",
    "\n",
    "    return pout, qout\n",
    "\n",
    "def model_prediction_noresqa_mos(test_feat, nmr_feat):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        score = model(nmr_feat,test_feat).detach().cpu().numpy()[0]\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# reading audio clips\n",
    "def audio_loading(path,sampling_rate=16000):\n",
    "\n",
    "    audio, fs = librosa.load(path)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    if fs != sampling_rate:\n",
    "        audio = librosa.resample(audio,fs,sampling_rate)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "# function checking if the size of the inputs are same. If not, then the reference audio's size is adjusted\n",
    "def check_size(audio_ref,audio_test):\n",
    "\n",
    "    if len(audio_ref) > len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "\n",
    "    elif len(audio_ref) < len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        while len(audio_test) > len(audio_ref):\n",
    "            audio_ref = np.append(audio_ref, audio_ref)\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "\n",
    "    return audio_ref, audio_test\n",
    "\n",
    "\n",
    "# audio loading and feature extraction\n",
    "def feats_loading(test_path, ref_path=None, noresqa_or_noresqaMOS = 0):\n",
    "\n",
    "    if noresqa_or_noresqaMOS == 0 or noresqa_or_noresqaMOS == 1:\n",
    "\n",
    "        audio_ref = audio_loading(ref_path)\n",
    "        audio_test = audio_loading(test_path)\n",
    "        audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "        if noresqa_or_noresqaMOS == 0:\n",
    "            ref_feat = extract_stft(audio_ref)\n",
    "            test_feat = extract_stft(audio_test)\n",
    "            return ref_feat,test_feat\n",
    "        else:\n",
    "            return audio_ref, audio_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from externals.metrics.Noresqa.model import NORESQA\n",
    "import fairseq\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda:{}\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/hydra/core/plugins.py:202: UserWarning: \n",
      "\tError importing 'hydra_plugins.hydra_colorlog'.\n",
      "\tPlugin is incompatible with this Hydra version or buggy.\n",
      "\tRecommended to uninstall or upgrade plugin.\n",
      "\t\tImportError : cannot import name 'SearchPathPlugin' from 'hydra.plugins' (/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/hydra/plugins/__init__.py)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = NORESQA(metric_type=1,config_path=\"/home/filip/speech_metrics_eval/externals/metrics/Noresqa/models/wav2vec_small.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11106/3491174430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maudio_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_stft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "audio_test = audio_loading(df.iloc[0].path)\n",
    "test_feat = extract_stft(audio_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmr_feat,test_feat = feats_loading(df.iloc[0].path, df.iloc[0].path, noresqa_or_noresqaMOS = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_test,sr = torchaudio.load(df.iloc[0].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "#del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 184504])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(audio_test).float().to(device).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 553509])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db</th>\n",
       "      <th>con</th>\n",
       "      <th>file</th>\n",
       "      <th>con_description</th>\n",
       "      <th>filename_deg</th>\n",
       "      <th>filename_ref</th>\n",
       "      <th>source</th>\n",
       "      <th>lang</th>\n",
       "      <th>votes</th>\n",
       "      <th>mos</th>\n",
       "      <th>...</th>\n",
       "      <th>dis</th>\n",
       "      <th>loud</th>\n",
       "      <th>noi_std</th>\n",
       "      <th>col_std</th>\n",
       "      <th>dis_std</th>\n",
       "      <th>loud_std</th>\n",
       "      <th>mos_std</th>\n",
       "      <th>filepath_deg</th>\n",
       "      <th>filepath_ref</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11670</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>9939.0</td>\n",
       "      <td>9939.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c09939_2_664_2_7_001-ch6-speaker_seg20.wav</td>\n",
       "      <td>2_664_2_7_001-ch6-speaker_seg20.wav</td>\n",
       "      <td>AusTalk</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.101412</td>\n",
       "      <td>1.155794</td>\n",
       "      <td>0.849365</td>\n",
       "      <td>1.422645</td>\n",
       "      <td>1.421229</td>\n",
       "      <td>0.140417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c09939_2_664_2_7_001-ch6-s...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/2_664_2_7_001-ch6-speaker_...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c01953_nom_08784_01631596649_seg.wav</td>\n",
       "      <td>nom_08784_01631596649_seg.wav</td>\n",
       "      <td>UKIRE</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.155966</td>\n",
       "      <td>3.347070</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.747127</td>\n",
       "      <td>0.092728</td>\n",
       "      <td>1.609947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c01953_nom_08784_016315966...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/nom_08784_01631596649_seg.wav</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5187</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c03456_3_775_2_7_001-ch6-speaker_seg50.wav</td>\n",
       "      <td>3_775_2_7_001-ch6-speaker_seg50.wav</td>\n",
       "      <td>AusTalk</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.050490</td>\n",
       "      <td>1.961792</td>\n",
       "      <td>1.757422</td>\n",
       "      <td>0.139913</td>\n",
       "      <td>1.217594</td>\n",
       "      <td>0.911812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c03456_3_775_2_7_001-ch6-s...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/3_775_2_7_001-ch6-speaker_...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12885</th>\n",
       "      <td>NISQA_VAL_SIM</td>\n",
       "      <td>954.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c0954_book_09628_chp_0013_reader_05511_9_seg.wav</td>\n",
       "      <td>book_09628_chp_0013_reader_05511_9_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.547234</td>\n",
       "      <td>2.225848</td>\n",
       "      <td>0.892520</td>\n",
       "      <td>0.246701</td>\n",
       "      <td>1.356060</td>\n",
       "      <td>1.184918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_VAL_SIM/deg/c0954_book_09628_chp_0013_re...</td>\n",
       "      <td>NISQA_VAL_SIM/ref/book_09628_chp_0013_reader_0...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>7777.0</td>\n",
       "      <td>7777.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c07777_sof_04766_00599431030_seg.wav</td>\n",
       "      <td>sof_04766_00599431030_seg.wav</td>\n",
       "      <td>UKIRE</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.246123</td>\n",
       "      <td>3.143344</td>\n",
       "      <td>0.168104</td>\n",
       "      <td>1.076841</td>\n",
       "      <td>0.716838</td>\n",
       "      <td>1.469859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c07777_sof_04766_005994310...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/sof_04766_00599431030_seg.wav</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6433</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c04702_book_03865_chp_0016_reader_03497_73_seg...</td>\n",
       "      <td>book_03865_chp_0016_reader_03497_73_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.770335</td>\n",
       "      <td>4.493423</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>0.336687</td>\n",
       "      <td>0.178099</td>\n",
       "      <td>0.367816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c04702_book_03865_chp_0016...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_03865_chp_0016_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>6239.0</td>\n",
       "      <td>6239.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c06239_book_01454_chp_0004_reader_10920_1_seg.wav</td>\n",
       "      <td>book_01454_chp_0004_reader_10920_1_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.744082</td>\n",
       "      <td>4.224199</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.265183</td>\n",
       "      <td>0.211983</td>\n",
       "      <td>0.524245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c06239_book_01454_chp_0004...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_01454_chp_0004_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>2906.0</td>\n",
       "      <td>2906.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c02906_book_07907_chp_0003_reader_02514_11_seg...</td>\n",
       "      <td>book_07907_chp_0003_reader_02514_11_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.699517</td>\n",
       "      <td>4.757853</td>\n",
       "      <td>0.405287</td>\n",
       "      <td>0.304674</td>\n",
       "      <td>0.363673</td>\n",
       "      <td>0.253140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c02906_book_07907_chp_0003...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_07907_chp_0003_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>4758.0</td>\n",
       "      <td>4758.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c04758_book_02922_chp_0034_reader_06496_42_seg...</td>\n",
       "      <td>book_02922_chp_0034_reader_06496_42_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.827712</td>\n",
       "      <td>4.837205</td>\n",
       "      <td>0.093347</td>\n",
       "      <td>0.114373</td>\n",
       "      <td>0.104091</td>\n",
       "      <td>0.098601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c04758_book_02922_chp_0034...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_02922_chp_0034_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c02900_book_03191_chp_0009_reader_00172_9_seg.wav</td>\n",
       "      <td>book_03191_chp_0009_reader_00172_9_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.690538</td>\n",
       "      <td>4.619894</td>\n",
       "      <td>0.201594</td>\n",
       "      <td>0.335522</td>\n",
       "      <td>0.148979</td>\n",
       "      <td>0.343060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c02900_book_03191_chp_0009...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_03191_chp_0009_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14432 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    db     con    file con_description  \\\n",
       "11670  NISQA_TRAIN_SIM  9939.0  9939.0       simulated   \n",
       "3684   NISQA_TRAIN_SIM  1953.0  1953.0       simulated   \n",
       "5187   NISQA_TRAIN_SIM  3456.0  3456.0       simulated   \n",
       "12885    NISQA_VAL_SIM   954.0   954.0       simulated   \n",
       "9508   NISQA_TRAIN_SIM  7777.0  7777.0       simulated   \n",
       "...                ...     ...     ...             ...   \n",
       "6433   NISQA_TRAIN_SIM  4702.0  4702.0       simulated   \n",
       "7970   NISQA_TRAIN_SIM  6239.0  6239.0       simulated   \n",
       "4637   NISQA_TRAIN_SIM  2906.0  2906.0       simulated   \n",
       "6489   NISQA_TRAIN_SIM  4758.0  4758.0       simulated   \n",
       "4631   NISQA_TRAIN_SIM  2900.0  2900.0       simulated   \n",
       "\n",
       "                                            filename_deg  \\\n",
       "11670         c09939_2_664_2_7_001-ch6-speaker_seg20.wav   \n",
       "3684                c01953_nom_08784_01631596649_seg.wav   \n",
       "5187          c03456_3_775_2_7_001-ch6-speaker_seg50.wav   \n",
       "12885   c0954_book_09628_chp_0013_reader_05511_9_seg.wav   \n",
       "9508                c07777_sof_04766_00599431030_seg.wav   \n",
       "...                                                  ...   \n",
       "6433   c04702_book_03865_chp_0016_reader_03497_73_seg...   \n",
       "7970   c06239_book_01454_chp_0004_reader_10920_1_seg.wav   \n",
       "4637   c02906_book_07907_chp_0003_reader_02514_11_seg...   \n",
       "6489   c04758_book_02922_chp_0034_reader_06496_42_seg...   \n",
       "4631   c02900_book_03191_chp_0009_reader_00172_9_seg.wav   \n",
       "\n",
       "                                      filename_ref   source lang  votes  mos  \\\n",
       "11670          2_664_2_7_001-ch6-speaker_seg20.wav  AusTalk   en    5.0  1.0   \n",
       "3684                 nom_08784_01631596649_seg.wav    UKIRE   en    5.0  1.0   \n",
       "5187           3_775_2_7_001-ch6-speaker_seg50.wav  AusTalk   en    4.0  1.0   \n",
       "12885   book_09628_chp_0013_reader_05511_9_seg.wav      DNS   en    5.0  1.0   \n",
       "9508                 sof_04766_00599431030_seg.wav    UKIRE   en    5.0  1.0   \n",
       "...                                            ...      ...  ...    ...  ...   \n",
       "6433   book_03865_chp_0016_reader_03497_73_seg.wav      DNS   en    5.0  5.0   \n",
       "7970    book_01454_chp_0004_reader_10920_1_seg.wav      DNS   en    4.0  5.0   \n",
       "4637   book_07907_chp_0003_reader_02514_11_seg.wav      DNS   en    5.0  5.0   \n",
       "6489   book_02922_chp_0034_reader_06496_42_seg.wav      DNS   en    5.0  5.0   \n",
       "4631    book_03191_chp_0009_reader_00172_9_seg.wav      DNS   en    5.0  5.0   \n",
       "\n",
       "       ...       dis      loud   noi_std   col_std   dis_std  loud_std  \\\n",
       "11670  ...  2.101412  1.155794  0.849365  1.422645  1.421229  0.140417   \n",
       "3684   ...  1.155966  3.347070  0.977778  0.747127  0.092728  1.609947   \n",
       "5187   ...  2.050490  1.961792  1.757422  0.139913  1.217594  0.911812   \n",
       "12885  ...  3.547234  2.225848  0.892520  0.246701  1.356060  1.184918   \n",
       "9508   ...  2.246123  3.143344  0.168104  1.076841  0.716838  1.469859   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "6433   ...  4.770335  4.493423  0.172042  0.336687  0.178099  0.367816   \n",
       "7970   ...  4.744082  4.224199  0.275500  0.265183  0.211983  0.524245   \n",
       "4637   ...  4.699517  4.757853  0.405287  0.304674  0.363673  0.253140   \n",
       "6489   ...  4.827712  4.837205  0.093347  0.114373  0.104091  0.098601   \n",
       "4631   ...  4.690538  4.619894  0.201594  0.335522  0.148979  0.343060   \n",
       "\n",
       "       mos_std                                       filepath_deg  \\\n",
       "11670      0.0  NISQA_TRAIN_SIM/deg/c09939_2_664_2_7_001-ch6-s...   \n",
       "3684       0.0  NISQA_TRAIN_SIM/deg/c01953_nom_08784_016315966...   \n",
       "5187       0.0  NISQA_TRAIN_SIM/deg/c03456_3_775_2_7_001-ch6-s...   \n",
       "12885      0.0  NISQA_VAL_SIM/deg/c0954_book_09628_chp_0013_re...   \n",
       "9508       0.0  NISQA_TRAIN_SIM/deg/c07777_sof_04766_005994310...   \n",
       "...        ...                                                ...   \n",
       "6433       0.0  NISQA_TRAIN_SIM/deg/c04702_book_03865_chp_0016...   \n",
       "7970       0.0  NISQA_TRAIN_SIM/deg/c06239_book_01454_chp_0004...   \n",
       "4637       0.0  NISQA_TRAIN_SIM/deg/c02906_book_07907_chp_0003...   \n",
       "6489       0.0  NISQA_TRAIN_SIM/deg/c04758_book_02922_chp_0034...   \n",
       "4631       0.0  NISQA_TRAIN_SIM/deg/c02900_book_03191_chp_0009...   \n",
       "\n",
       "                                            filepath_ref  \\\n",
       "11670  NISQA_TRAIN_SIM/ref/2_664_2_7_001-ch6-speaker_...   \n",
       "3684   NISQA_TRAIN_SIM/ref/nom_08784_01631596649_seg.wav   \n",
       "5187   NISQA_TRAIN_SIM/ref/3_775_2_7_001-ch6-speaker_...   \n",
       "12885  NISQA_VAL_SIM/ref/book_09628_chp_0013_reader_0...   \n",
       "9508   NISQA_TRAIN_SIM/ref/sof_04766_00599431030_seg.wav   \n",
       "...                                                  ...   \n",
       "6433   NISQA_TRAIN_SIM/ref/book_03865_chp_0016_reader...   \n",
       "7970   NISQA_TRAIN_SIM/ref/book_01454_chp_0004_reader...   \n",
       "4637   NISQA_TRAIN_SIM/ref/book_07907_chp_0003_reader...   \n",
       "6489   NISQA_TRAIN_SIM/ref/book_02922_chp_0034_reader...   \n",
       "4631   NISQA_TRAIN_SIM/ref/book_03191_chp_0009_reader...   \n",
       "\n",
       "                                                    path  \n",
       "11670  /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "3684   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "5187   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "12885  /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "9508   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "...                                                  ...  \n",
       "6433   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "7970   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "4637   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "6489   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "4631   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "\n",
       "[14432 rows x 22 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"mos\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6858/2921841177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.from_numpy(audio_test).float().to(device).unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "# torch.from_numpy(audio_test).float().to(device).unsqueeze(0)\n",
    "t = torch.stack([torch.from_numpy(audio_test).float().to(device),torch.from_numpy(audio_test).float().to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_audio,sr = torchaudio.load(df.sort_values(\"mos\").iloc[0].path)\n",
    "noisy_audio,sr = torchaudio.load(df.sort_values(\"mos\").iloc[-1].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.stack([noisy_audio[:,:10000],noisy_audio[:,:10000]]).to(device),\n",
    "torch.stack([clean_audio[:,:10000],clean_audio[:,:10000]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noisy_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11106/2039743769.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m out = model(torch.stack([noisy_audio[:,:10000],noisy_audio[:,:10000]]).to(device),\n\u001b[0m\u001b[1;32m      2\u001b[0m torch.stack([clean_audio[:,:10000],clean_audio[:,:10000]]).to(device))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noisy_audio' is not defined"
     ]
    }
   ],
   "source": [
    "out = model(torch.stack([noisy_audio[:,:10000],noisy_audio[:,:10000]]).to(device),\n",
    "torch.stack([clean_audio[:,:10000],clean_audio[:,:10000]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11106/733800577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "tp = torch.tensor([int(df.sort_values(\"mos\").iloc[0].mos < df.sort_values(\"mos\").iloc[-1].mos), int(df.sort_values(\"mos\").iloc[0].mos > df.sort_values(\"mos\").iloc[-1].mos)])\n",
    "tp = torch.stack([tp,tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0673, 2.0540], device='cuda:1', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.pow(out[1].cpu(),iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4098, -2.3022], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.pow(tp,iter) * torch.log(torch.pow(out[1].cpu(),iter)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.pow(out[1].cpu(),iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.0427, -6.1038], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log(torch.pow(out[1].cpu(),iter)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = torch.tensor([1.,2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(-tp,iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 2,steps=2)#.to(concat.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_loss(output_preference, true_preference):\n",
    "    iter = torch.tensor([1.,2.]).to(output_preference.device)\n",
    "    return torch.sum(torch.pow(true_preference,iter) * torch.log(torch.pow(output_preference,iter)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_rating_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/data/speech_metrics_eval/NISQA_Corpus/NISQA_corpus_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "torchaudio.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n"
     ]
    }
   ],
   "source": [
    "#import librosa\n",
    "import torchaudio\n",
    "for db_type in df[\"db\"].unique():\n",
    "    a,sr = torchaudio.load(df[df[\"db\"] == db_type].iloc[0].path)\n",
    "    print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/work/data/speech_metrics_eval/NISQA_Corpus\"\n",
    "df['path'] = df.apply(lambda x: f\"{dir_path}/{x['db']}/deg/{x['filename_deg']}\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, entries):\n",
    "        self.entries = entries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries.iloc[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bacace66927486d9dce3112ba76c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f1444ac2b94fc885ff9d3b9d3ffbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1471cb135b4790b18307e99b607634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55baa9884cd64683bb7e1d19090542ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb18580842f848bcb4966f9995daac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ed62c4f2634044aeee0f3d462d85ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/360M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d52d2489de4d46af6fc8b1b00ca0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr_dummy/clean to /home/filip/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8e1a5bb6654d96b8a4ccbba65a0aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a996ffa57d849b2977f56ddb448977a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b5a6581ea640738d1031e39e4969e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34476bc03b2940fe897b66e2512bb2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset librispeech_asr_dummy downloaded and prepared to /home/filip/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    " from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    " from datasets import load_dataset\n",
    " import torch\n",
    " \n",
    " # load model and tokenizer\n",
    " processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    " model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "     \n",
    " # load dummy dataset and read soundfiles\n",
    " ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    " \n",
    " # tokenize\n",
    " input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    " \n",
    " # retrieve logits\n",
    " logits = model(input_values).logits\n",
    " \n",
    " # take argmax and decode\n",
    " predicted_ids = torch.argmax(logits, dim=-1)\n",
    " transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A MAN SAID TO THE UNIVERSE SIR I EXIST']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['db', 'con', 'file', 'con_description', 'filename_deg', 'filename_ref',\n",
       "       'source', 'lang', 'votes', 'mos', 'noi', 'col', 'dis', 'loud',\n",
       "       'noi_std', 'col_std', 'dis_std', 'loud_std', 'mos_std', 'filepath_deg',\n",
       "       'filepath_ref', 'path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>MOS</th>\n",
       "      <th>std</th>\n",
       "      <th>95%CI</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25765194_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.875</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25634417_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>0.370405</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25705754_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.250</td>\n",
       "      <td>0.886405</td>\n",
       "      <td>0.614248</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25706727_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.523832</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25800071_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.875</td>\n",
       "      <td>0.834523</td>\n",
       "      <td>0.578295</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58704</th>\n",
       "      <td>30083997_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.619806</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58705</th>\n",
       "      <td>29542672_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.800</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58706</th>\n",
       "      <td>30464540_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>2.800</td>\n",
       "      <td>1.095445</td>\n",
       "      <td>0.960200</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58707</th>\n",
       "      <td>28950987_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.800</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>0.733365</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58708</th>\n",
       "      <td>28354886_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.200</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>0.733365</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58709 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename    MOS       std     95%CI  votes\n",
       "0      25765194_MinPolqaSfbRecording_cut.wav  3.875  0.353553  0.245000      8\n",
       "1      25634417_MinPolqaSfbRecording_cut.wav  1.500  0.534522  0.370405      8\n",
       "2      25705754_MinPolqaSfbRecording_cut.wav  3.250  0.886405  0.614248      8\n",
       "3      25706727_MinPolqaSfbRecording_cut.wav  3.000  0.755929  0.523832      8\n",
       "4      25800071_MinPolqaSfbRecording_cut.wav  3.875  0.834523  0.578295      8\n",
       "...                                      ...    ...       ...       ...    ...\n",
       "58704  30083997_MinPolqaSfbRecording_cut.wav  4.000  0.707107  0.619806      5\n",
       "58705  29542672_MinPolqaSfbRecording_cut.wav  3.800  0.447214  0.392000      5\n",
       "58706  30464540_MinPolqaSfbRecording_cut.wav  2.800  1.095445  0.960200      5\n",
       "58707  28950987_MinPolqaSfbRecording_cut.wav  3.800  0.836660  0.733365      5\n",
       "58708  28354886_MinPolqaSfbRecording_cut.wav  3.200  0.836660  0.733365      5\n",
       "\n",
       "[58709 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"/work/data/speech_metrics_eval/pstn_train/pstn_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f29d6d60ac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.main_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORESQA(\n",
       "  (main_model): MosPredictor(\n",
       "    (ssl_model): Wav2Vec2Model(\n",
       "      (feature_extractor): ConvFeatureExtractionModel(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            (3): GELU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "      (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "      (quantizer): None\n",
       "      (project_q): None\n",
       "      (encoder): TransformerEncoder(\n",
       "        (pos_conv): Sequential(\n",
       "          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (1): SamePad()\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_proj): None\n",
       "    )\n",
       "  )\n",
       "  (linear_layer): Linear(in_features=768, out_features=32, bias=True)\n",
       "  (quantification): PoolAtt(\n",
       "    (linear1): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (linear2): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       "  (preference): PoolAtt(\n",
       "    (linear1): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (linear2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43eb8f28c749eccd0c3ad7538053c89983745c874e5d91c778c1295a53c7d146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
