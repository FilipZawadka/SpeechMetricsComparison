{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/filip/samsara/elysium/externals/Noresqa\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import librosa\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from model import NORESQA\n",
    "from scipy import signal\n",
    "import glob\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORESQA(\n",
       "  (base_encoder): base_encoder(\n",
       "    (modelA): model_dimred(\n",
       "      (modules1): ModuleList(\n",
       "        (0): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(2, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Conv2d(16, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (5): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(2, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (modelB): model_dimred(\n",
       "      (modules1): ModuleList(\n",
       "        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Conv2d(16, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (5): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (modelC): model_dimred(\n",
       "      (modules1): ModuleList(\n",
       "        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Conv2d(16, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (5): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (modelD): model_dimred(\n",
       "      (modules1): ModuleList(\n",
       "        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Conv2d(16, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (5): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (base_encoder_2): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.2, inplace=False)\n",
       "          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (4): ReLU()\n",
       "          (5): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): TemporalBlock(\n",
       "        (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.2, inplace=False)\n",
       "          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "          (4): ReLU()\n",
       "          (5): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): TemporalBlock(\n",
       "        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.2, inplace=False)\n",
       "          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "          (4): ReLU()\n",
       "          (5): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (3): TemporalBlock(\n",
       "        (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.2, inplace=False)\n",
       "          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "          (4): ReLU()\n",
       "          (5): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (which_clean): which_clean(\n",
       "    (encoder): ModuleList(\n",
       "      (0): Conv1d(128, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): Conv1d(32, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (2): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    )\n",
       "    (ebatch): ModuleList(\n",
       "      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dp): ModuleList(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (how_snr_sdr): how_snr(\n",
       "    (encoder): ModuleList(\n",
       "      (0): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (2): Conv1d(32, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    )\n",
       "    (ebatch): ModuleList(\n",
       "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dp): ModuleList(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (how_snr_snr): how_snr_snr(\n",
       "    (encoder): ModuleList(\n",
       "      (0): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (2): Conv1d(32, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    )\n",
       "    (ebatch): ModuleList(\n",
       "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dp): ModuleList(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (CE): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:{}\".format(0))\n",
    "model = NORESQA()\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stft(audio, sampling_rate = 16000):\n",
    "\n",
    "    f, t, Sxx = signal.stft(audio, sampling_rate, window='hann',nperseg=512,noverlap=256,nfft=512)\n",
    "    Sxx = Sxx[:256,:]\n",
    "\n",
    "    feat = np.concatenate((np.abs(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1]), np.angle(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1])), axis=2)\n",
    "\n",
    "    return feat \n",
    "\n",
    "# function doing prediction\n",
    "def model_prediction(test_feat, nmr_feat):\n",
    "\n",
    "    intervals_sdr = np.arange(0.5,40,1) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ranking_frame,sdr_frame,snr_frame = model(test_feat.permute(0,3,2,1),nmr_feat.permute(0,3,2,1))\n",
    "        # preference task prediction\n",
    "        \n",
    "        ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "        pout = ranking[0][0]\n",
    "\n",
    "        # quantification task\n",
    "        sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "        qout = sdr.sum()\n",
    "\n",
    "    return pout, qout\n",
    "\n",
    "# function checking if the size of the inputs are same. If not, then the reference audio's size is adjusted\n",
    "def check_size(audio_ref,audio_test):\n",
    "    \n",
    "    if len(audio_ref) > len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "        \n",
    "    elif len(audio_ref) < len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        while len(audio_test) > len(audio_ref):\n",
    "            audio_ref = np.append(audio_ref, audio_ref)\n",
    "        audio_ref = audio_ref[:len(audio_test)] \n",
    "    \n",
    "    return audio_ref, audio_test\n",
    "\n",
    "# reading audio clips\n",
    "def audio_loading(path,sampling_rate=16000):\n",
    "\n",
    "    audio, fs = librosa.load(path)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    if fs != sampling_rate:\n",
    "        audio = librosa.resample(audio,fs,sampling_rate)\n",
    "\n",
    "    return audio\n",
    "\n",
    "# top level function combining loading, and feature extraction\n",
    "def feats_loading(ref_path,test_path):\n",
    "    \n",
    "    audio_ref = audio_loading(ref_path)\n",
    "    audio_test = audio_loading(test_path)\n",
    "\n",
    "    audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "    ref_feat = extract_stft(audio_ref)\n",
    "    test_feat = extract_stft(audio_test)\n",
    "\n",
    "    return ref_feat,test_feat\n",
    "\n",
    "def inference_file(nmr,test_file):\n",
    "    nmr_feat,test_feat = feats_loading(nmr,test_file)\n",
    "    test_feat = torch.from_numpy(test_feat).float().to(device).unsqueeze(0)\n",
    "    nmr_feat = torch.from_numpy(nmr_feat).float().to(device).unsqueeze(0)\n",
    "\n",
    "    return model_prediction(test_feat,nmr_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feats_loading('/work/data/timit/data/TEST/DR1/FAKS0/SI943.WAV',\n",
    "#glob.glob(\"/work/data/speech_metrics_eval/TencentCorups/withReverberationTrainDev/*\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1976/230907824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Loading checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./externals/metrics/Noresqa/models/model_noresqa_mos.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_base'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    775\u001b[0m             \"functionality.\")\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/filip/samsara/elysium/externals/Noresqa\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import librosa as librosa\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from model import NORESQA\n",
    "from scipy import signal\n",
    "import glob\n",
    "import ntpath\n",
    "\n",
    "model = NORESQA()\n",
    "\n",
    "# Loading checkpoint\n",
    "model_checkpoint_path = './externals/metrics/Noresqa/models/model_noresqa_mos.pth'\n",
    "state = torch.load(model_checkpoint_path,map_location=\"cpu\")['state_base']\n",
    "\n",
    "pretrained_dict = {}\n",
    "for k, v in state.items():\n",
    "    if 'module' in k:\n",
    "        pretrained_dict[k.replace('module.','')]=v\n",
    "    else:\n",
    "        pretrained_dict[k]=v\n",
    "model_dict = model.state_dict()\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(pretrained_dict)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:{}\".format(0))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sfmax = nn.Softmax(dim=1)\n",
    "\n",
    "# function extraction stft\n",
    "def extract_stft(audio, sampling_rate = 16000):\n",
    "\n",
    "    f, t, Sxx = signal.stft(audio, sampling_rate, window='hann',nperseg=512,noverlap=256,nfft=512)\n",
    "    Sxx = Sxx[:256,:]\n",
    "\n",
    "    feat = np.concatenate((np.abs(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1]), np.angle(Sxx).reshape([Sxx.shape[0],Sxx.shape[1],1])), axis=2)\n",
    "\n",
    "    return feat \n",
    "\n",
    "# function doing prediction\n",
    "def model_prediction(test_feat, nmr_feat):\n",
    "\n",
    "    intervals_sdr = np.arange(0.5,40,1) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ranking_frame,sdr_frame,snr_frame = model(test_feat.permute(0,3,2,1),nmr_feat.permute(0,3,2,1))\n",
    "        # preference task prediction\n",
    "        \n",
    "        ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "        pout = ranking[0][0]\n",
    "\n",
    "        # quantification task\n",
    "        sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "        qout = sdr.sum()\n",
    "\n",
    "    return pout, qout\n",
    "\n",
    "# function checking if the size of the inputs are same. If not, then the reference audio's size is adjusted\n",
    "def check_size(audio_ref,audio_test):\n",
    "    \n",
    "    if len(audio_ref) > len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "        \n",
    "    elif len(audio_ref) < len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        while len(audio_test) > len(audio_ref):\n",
    "            audio_ref = np.append(audio_ref, audio_ref)\n",
    "        audio_ref = audio_ref[:len(audio_test)] \n",
    "    \n",
    "    return audio_ref, audio_test\n",
    "\n",
    "# reading audio clips\n",
    "def audio_loading(path,sampling_rate=16000):\n",
    "\n",
    "    audio, fs = librosa.load(path)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    if fs != sampling_rate:\n",
    "        audio = librosa.resample(audio,fs,sampling_rate)\n",
    "\n",
    "    return audio\n",
    "\n",
    "# top level function combining loading, and feature extraction\n",
    "def feats_loading(ref_path,test_path):\n",
    "    \n",
    "    audio_ref = audio_loading(ref_path)\n",
    "    audio_test = audio_loading(test_path)\n",
    "\n",
    "    audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "    ref_feat = extract_stft(audio_ref)\n",
    "    test_feat = extract_stft(audio_test)\n",
    "\n",
    "    return ref_feat,test_feat\n",
    "\n",
    "def inference_file(nmr,test_file):\n",
    "    nmr_feat,test_feat = feats_loading(nmr,test_file)\n",
    "    test_feat = torch.from_numpy(test_feat).float().to(device).unsqueeze(0)\n",
    "    nmr_feat = torch.from_numpy(nmr_feat).float().to(device).unsqueeze(0)\n",
    "\n",
    "    return model_prediction(test_feat,nmr_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durations dont match. Adjusting duration of reference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9049/1267914694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mranking_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msdr_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnr_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# preference task prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samsara/elysium/externals/Noresqa/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_encoder_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samsara/elysium/externals/Noresqa/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samsara/elysium/externals/Noresqa/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "x1, x2 = feats_loading('/work/data/timit/data/TEST/DR1/FAKS0/SI943.WAV',\n",
    "glob.glob(\"/work/data/speech_metrics_eval/TencentCorups/withReverberationTrainDev/*\")[0])\n",
    "x1 = torch.from_numpy(x1).float().to(device).unsqueeze(0)\n",
    "x2 = torch.from_numpy(x2).float().to(device).unsqueeze(0)\n",
    "\n",
    "intervals_sdr = np.arange(0.5,40,1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    ranking_frame,sdr_frame,snr_frame = model(x1.permute(0,3,2,1),x2.permute(0,3,2,1))\n",
    "    # preference task prediction\n",
    "    \n",
    "    ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "    pout = ranking[0][0]\n",
    "\n",
    "    # quantification task\n",
    "    sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "    qout = sdr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/filip/samsara/elysium/externals/Noresqa\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import librosa as librosa\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from model import NORESQA\n",
    "from scipy import signal\n",
    "\n",
    "CONFIG_PATH = 'externals/metrics/Noresqa/models/wav2vec_small.pt'\n",
    "metric_type = 1\n",
    "load_checkpoint = False\n",
    "GPU_id = 0\n",
    "# Noresqa model\n",
    "model = NORESQA(40,40,1 #,CONFIG_PATH\n",
    ")\n",
    "if load_checkpoint:\n",
    "# Loading checkpoint\n",
    "    if metric_type==0:\n",
    "        model_checkpoint_path = 'externals/metrics/Noresqa/models/model_noresqa.pth'\n",
    "        state = torch.load(model_checkpoint_path,map_location=\"cpu\")['state_base']\n",
    "    elif metric_type == 1:\n",
    "        model_checkpoint_path = 'externals/metrics/Noresqa/models/model_noresqa_mos.pth'\n",
    "        state = torch.load(model_checkpoint_path,map_location=\"cpu\")['state_dict']\n",
    "\n",
    "    pretrained_dict = {}\n",
    "    for k, v in state.items():\n",
    "        if 'module' in k:\n",
    "            pretrained_dict[k.replace('module.','')]=v\n",
    "        else:\n",
    "            pretrained_dict[k]=v\n",
    "    model_dict = model.state_dict()\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict)\n",
    "\n",
    "# change device as needed\n",
    "# device\n",
    "if GPU_id >=0 and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:{}\".format(GPU_id))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sfmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function extraction stft\n",
    "def extract_stft(audio, sampling_rate = 16000):\n",
    "\n",
    "    fx, tx, stft_out = signal.stft(audio, sampling_rate, window='hann',nperseg=512,noverlap=256,nfft=512)\n",
    "    stft_out = stft_out[:256,:]\n",
    "    feat = np.concatenate((np.abs(stft_out).reshape([stft_out.shape[0],stft_out.shape[1],1]), np.angle(stft_out).reshape([stft_out.shape[0],stft_out.shape[1],1])), axis=2)\n",
    "    return feat\n",
    "\n",
    "# noresqa and noresqa-mos prediction calls\n",
    "def model_prediction_noresqa(test_feat, nmr_feat):\n",
    "\n",
    "    intervals_sdr = np.arange(0.5,40,1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ranking_frame,sdr_frame,snr_frame = model(test_feat.permute(0,3,2,1),nmr_feat.permute(0,3,2,1))\n",
    "        # preference task prediction\n",
    "        ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "        pout = ranking[0][0]\n",
    "        # quantification task\n",
    "        sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "        qout = sdr.sum()\n",
    "\n",
    "    return pout, qout\n",
    "\n",
    "def model_prediction_noresqa_mos(test_feat, nmr_feat):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        score = model(nmr_feat,test_feat).detach().cpu().numpy()[0]\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# reading audio clips\n",
    "def audio_loading(path,sampling_rate=16000):\n",
    "\n",
    "    audio, fs = librosa.load(path)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    if fs != sampling_rate:\n",
    "        audio = librosa.resample(audio,fs,sampling_rate)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "# function checking if the size of the inputs are same. If not, then the reference audio's size is adjusted\n",
    "def check_size(audio_ref,audio_test):\n",
    "\n",
    "    if len(audio_ref) > len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "\n",
    "    elif len(audio_ref) < len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        while len(audio_test) > len(audio_ref):\n",
    "            audio_ref = np.append(audio_ref, audio_ref)\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "\n",
    "    return audio_ref, audio_test\n",
    "\n",
    "\n",
    "# audio loading and feature extraction\n",
    "def feats_loading(test_path, ref_path=None, noresqa_or_noresqaMOS = 0):\n",
    "\n",
    "    if noresqa_or_noresqaMOS == 0 or noresqa_or_noresqaMOS == 1:\n",
    "\n",
    "        audio_ref = audio_loading(ref_path)\n",
    "        audio_test = audio_loading(test_path)\n",
    "        audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "        if noresqa_or_noresqaMOS == 0:\n",
    "            ref_feat = extract_stft(audio_ref)\n",
    "            test_feat = extract_stft(audio_test)\n",
    "            return ref_feat,test_feat\n",
    "        else:\n",
    "            return audio_ref, audio_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from externals.metrics.Noresqa.model import NORESQA\n",
    "import fairseq\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda:{}\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/hydra/core/plugins.py:202: UserWarning: \n",
      "\tError importing 'hydra_plugins.hydra_colorlog'.\n",
      "\tPlugin is incompatible with this Hydra version or buggy.\n",
      "\tRecommended to uninstall or upgrade plugin.\n",
      "\t\tImportError : cannot import name 'SearchPathPlugin' from 'hydra.plugins' (/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/hydra/plugins/__init__.py)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1976/3824924773.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNORESQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/filip/speech_metrics_eval/externals/metrics/Noresqa/models/wav2vec_small.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "model = NORESQA(metric_type=1,config_path=\"/home/filip/speech_metrics_eval/externals/metrics/Noresqa/models/wav2vec_small.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11106/3491174430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maudio_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_stft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "audio_test = audio_loading(df.iloc[0].path)\n",
    "test_feat = extract_stft(audio_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmr_feat,test_feat = feats_loading(df.iloc[0].path, df.iloc[0].path, noresqa_or_noresqaMOS = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_test,sr = torchaudio.load(df.iloc[0].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "#del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 184504])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(audio_test).float().to(device).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 553509])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db</th>\n",
       "      <th>con</th>\n",
       "      <th>file</th>\n",
       "      <th>con_description</th>\n",
       "      <th>filename_deg</th>\n",
       "      <th>filename_ref</th>\n",
       "      <th>source</th>\n",
       "      <th>lang</th>\n",
       "      <th>votes</th>\n",
       "      <th>mos</th>\n",
       "      <th>...</th>\n",
       "      <th>dis</th>\n",
       "      <th>loud</th>\n",
       "      <th>noi_std</th>\n",
       "      <th>col_std</th>\n",
       "      <th>dis_std</th>\n",
       "      <th>loud_std</th>\n",
       "      <th>mos_std</th>\n",
       "      <th>filepath_deg</th>\n",
       "      <th>filepath_ref</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11670</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>9939.0</td>\n",
       "      <td>9939.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c09939_2_664_2_7_001-ch6-speaker_seg20.wav</td>\n",
       "      <td>2_664_2_7_001-ch6-speaker_seg20.wav</td>\n",
       "      <td>AusTalk</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.101412</td>\n",
       "      <td>1.155794</td>\n",
       "      <td>0.849365</td>\n",
       "      <td>1.422645</td>\n",
       "      <td>1.421229</td>\n",
       "      <td>0.140417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c09939_2_664_2_7_001-ch6-s...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/2_664_2_7_001-ch6-speaker_...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c01953_nom_08784_01631596649_seg.wav</td>\n",
       "      <td>nom_08784_01631596649_seg.wav</td>\n",
       "      <td>UKIRE</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.155966</td>\n",
       "      <td>3.347070</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.747127</td>\n",
       "      <td>0.092728</td>\n",
       "      <td>1.609947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c01953_nom_08784_016315966...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/nom_08784_01631596649_seg.wav</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5187</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c03456_3_775_2_7_001-ch6-speaker_seg50.wav</td>\n",
       "      <td>3_775_2_7_001-ch6-speaker_seg50.wav</td>\n",
       "      <td>AusTalk</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.050490</td>\n",
       "      <td>1.961792</td>\n",
       "      <td>1.757422</td>\n",
       "      <td>0.139913</td>\n",
       "      <td>1.217594</td>\n",
       "      <td>0.911812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c03456_3_775_2_7_001-ch6-s...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/3_775_2_7_001-ch6-speaker_...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12885</th>\n",
       "      <td>NISQA_VAL_SIM</td>\n",
       "      <td>954.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c0954_book_09628_chp_0013_reader_05511_9_seg.wav</td>\n",
       "      <td>book_09628_chp_0013_reader_05511_9_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.547234</td>\n",
       "      <td>2.225848</td>\n",
       "      <td>0.892520</td>\n",
       "      <td>0.246701</td>\n",
       "      <td>1.356060</td>\n",
       "      <td>1.184918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_VAL_SIM/deg/c0954_book_09628_chp_0013_re...</td>\n",
       "      <td>NISQA_VAL_SIM/ref/book_09628_chp_0013_reader_0...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>7777.0</td>\n",
       "      <td>7777.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c07777_sof_04766_00599431030_seg.wav</td>\n",
       "      <td>sof_04766_00599431030_seg.wav</td>\n",
       "      <td>UKIRE</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.246123</td>\n",
       "      <td>3.143344</td>\n",
       "      <td>0.168104</td>\n",
       "      <td>1.076841</td>\n",
       "      <td>0.716838</td>\n",
       "      <td>1.469859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c07777_sof_04766_005994310...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/sof_04766_00599431030_seg.wav</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6433</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c04702_book_03865_chp_0016_reader_03497_73_seg...</td>\n",
       "      <td>book_03865_chp_0016_reader_03497_73_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.770335</td>\n",
       "      <td>4.493423</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>0.336687</td>\n",
       "      <td>0.178099</td>\n",
       "      <td>0.367816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c04702_book_03865_chp_0016...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_03865_chp_0016_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>6239.0</td>\n",
       "      <td>6239.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c06239_book_01454_chp_0004_reader_10920_1_seg.wav</td>\n",
       "      <td>book_01454_chp_0004_reader_10920_1_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.744082</td>\n",
       "      <td>4.224199</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.265183</td>\n",
       "      <td>0.211983</td>\n",
       "      <td>0.524245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c06239_book_01454_chp_0004...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_01454_chp_0004_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>2906.0</td>\n",
       "      <td>2906.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c02906_book_07907_chp_0003_reader_02514_11_seg...</td>\n",
       "      <td>book_07907_chp_0003_reader_02514_11_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.699517</td>\n",
       "      <td>4.757853</td>\n",
       "      <td>0.405287</td>\n",
       "      <td>0.304674</td>\n",
       "      <td>0.363673</td>\n",
       "      <td>0.253140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c02906_book_07907_chp_0003...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_07907_chp_0003_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>4758.0</td>\n",
       "      <td>4758.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c04758_book_02922_chp_0034_reader_06496_42_seg...</td>\n",
       "      <td>book_02922_chp_0034_reader_06496_42_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.827712</td>\n",
       "      <td>4.837205</td>\n",
       "      <td>0.093347</td>\n",
       "      <td>0.114373</td>\n",
       "      <td>0.104091</td>\n",
       "      <td>0.098601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c04758_book_02922_chp_0034...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_02922_chp_0034_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>NISQA_TRAIN_SIM</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>simulated</td>\n",
       "      <td>c02900_book_03191_chp_0009_reader_00172_9_seg.wav</td>\n",
       "      <td>book_03191_chp_0009_reader_00172_9_seg.wav</td>\n",
       "      <td>DNS</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.690538</td>\n",
       "      <td>4.619894</td>\n",
       "      <td>0.201594</td>\n",
       "      <td>0.335522</td>\n",
       "      <td>0.148979</td>\n",
       "      <td>0.343060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NISQA_TRAIN_SIM/deg/c02900_book_03191_chp_0009...</td>\n",
       "      <td>NISQA_TRAIN_SIM/ref/book_03191_chp_0009_reader...</td>\n",
       "      <td>/work/data/speech_metrics_eval/NISQA_Corpus/NI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14432 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    db     con    file con_description  \\\n",
       "11670  NISQA_TRAIN_SIM  9939.0  9939.0       simulated   \n",
       "3684   NISQA_TRAIN_SIM  1953.0  1953.0       simulated   \n",
       "5187   NISQA_TRAIN_SIM  3456.0  3456.0       simulated   \n",
       "12885    NISQA_VAL_SIM   954.0   954.0       simulated   \n",
       "9508   NISQA_TRAIN_SIM  7777.0  7777.0       simulated   \n",
       "...                ...     ...     ...             ...   \n",
       "6433   NISQA_TRAIN_SIM  4702.0  4702.0       simulated   \n",
       "7970   NISQA_TRAIN_SIM  6239.0  6239.0       simulated   \n",
       "4637   NISQA_TRAIN_SIM  2906.0  2906.0       simulated   \n",
       "6489   NISQA_TRAIN_SIM  4758.0  4758.0       simulated   \n",
       "4631   NISQA_TRAIN_SIM  2900.0  2900.0       simulated   \n",
       "\n",
       "                                            filename_deg  \\\n",
       "11670         c09939_2_664_2_7_001-ch6-speaker_seg20.wav   \n",
       "3684                c01953_nom_08784_01631596649_seg.wav   \n",
       "5187          c03456_3_775_2_7_001-ch6-speaker_seg50.wav   \n",
       "12885   c0954_book_09628_chp_0013_reader_05511_9_seg.wav   \n",
       "9508                c07777_sof_04766_00599431030_seg.wav   \n",
       "...                                                  ...   \n",
       "6433   c04702_book_03865_chp_0016_reader_03497_73_seg...   \n",
       "7970   c06239_book_01454_chp_0004_reader_10920_1_seg.wav   \n",
       "4637   c02906_book_07907_chp_0003_reader_02514_11_seg...   \n",
       "6489   c04758_book_02922_chp_0034_reader_06496_42_seg...   \n",
       "4631   c02900_book_03191_chp_0009_reader_00172_9_seg.wav   \n",
       "\n",
       "                                      filename_ref   source lang  votes  mos  \\\n",
       "11670          2_664_2_7_001-ch6-speaker_seg20.wav  AusTalk   en    5.0  1.0   \n",
       "3684                 nom_08784_01631596649_seg.wav    UKIRE   en    5.0  1.0   \n",
       "5187           3_775_2_7_001-ch6-speaker_seg50.wav  AusTalk   en    4.0  1.0   \n",
       "12885   book_09628_chp_0013_reader_05511_9_seg.wav      DNS   en    5.0  1.0   \n",
       "9508                 sof_04766_00599431030_seg.wav    UKIRE   en    5.0  1.0   \n",
       "...                                            ...      ...  ...    ...  ...   \n",
       "6433   book_03865_chp_0016_reader_03497_73_seg.wav      DNS   en    5.0  5.0   \n",
       "7970    book_01454_chp_0004_reader_10920_1_seg.wav      DNS   en    4.0  5.0   \n",
       "4637   book_07907_chp_0003_reader_02514_11_seg.wav      DNS   en    5.0  5.0   \n",
       "6489   book_02922_chp_0034_reader_06496_42_seg.wav      DNS   en    5.0  5.0   \n",
       "4631    book_03191_chp_0009_reader_00172_9_seg.wav      DNS   en    5.0  5.0   \n",
       "\n",
       "       ...       dis      loud   noi_std   col_std   dis_std  loud_std  \\\n",
       "11670  ...  2.101412  1.155794  0.849365  1.422645  1.421229  0.140417   \n",
       "3684   ...  1.155966  3.347070  0.977778  0.747127  0.092728  1.609947   \n",
       "5187   ...  2.050490  1.961792  1.757422  0.139913  1.217594  0.911812   \n",
       "12885  ...  3.547234  2.225848  0.892520  0.246701  1.356060  1.184918   \n",
       "9508   ...  2.246123  3.143344  0.168104  1.076841  0.716838  1.469859   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "6433   ...  4.770335  4.493423  0.172042  0.336687  0.178099  0.367816   \n",
       "7970   ...  4.744082  4.224199  0.275500  0.265183  0.211983  0.524245   \n",
       "4637   ...  4.699517  4.757853  0.405287  0.304674  0.363673  0.253140   \n",
       "6489   ...  4.827712  4.837205  0.093347  0.114373  0.104091  0.098601   \n",
       "4631   ...  4.690538  4.619894  0.201594  0.335522  0.148979  0.343060   \n",
       "\n",
       "       mos_std                                       filepath_deg  \\\n",
       "11670      0.0  NISQA_TRAIN_SIM/deg/c09939_2_664_2_7_001-ch6-s...   \n",
       "3684       0.0  NISQA_TRAIN_SIM/deg/c01953_nom_08784_016315966...   \n",
       "5187       0.0  NISQA_TRAIN_SIM/deg/c03456_3_775_2_7_001-ch6-s...   \n",
       "12885      0.0  NISQA_VAL_SIM/deg/c0954_book_09628_chp_0013_re...   \n",
       "9508       0.0  NISQA_TRAIN_SIM/deg/c07777_sof_04766_005994310...   \n",
       "...        ...                                                ...   \n",
       "6433       0.0  NISQA_TRAIN_SIM/deg/c04702_book_03865_chp_0016...   \n",
       "7970       0.0  NISQA_TRAIN_SIM/deg/c06239_book_01454_chp_0004...   \n",
       "4637       0.0  NISQA_TRAIN_SIM/deg/c02906_book_07907_chp_0003...   \n",
       "6489       0.0  NISQA_TRAIN_SIM/deg/c04758_book_02922_chp_0034...   \n",
       "4631       0.0  NISQA_TRAIN_SIM/deg/c02900_book_03191_chp_0009...   \n",
       "\n",
       "                                            filepath_ref  \\\n",
       "11670  NISQA_TRAIN_SIM/ref/2_664_2_7_001-ch6-speaker_...   \n",
       "3684   NISQA_TRAIN_SIM/ref/nom_08784_01631596649_seg.wav   \n",
       "5187   NISQA_TRAIN_SIM/ref/3_775_2_7_001-ch6-speaker_...   \n",
       "12885  NISQA_VAL_SIM/ref/book_09628_chp_0013_reader_0...   \n",
       "9508   NISQA_TRAIN_SIM/ref/sof_04766_00599431030_seg.wav   \n",
       "...                                                  ...   \n",
       "6433   NISQA_TRAIN_SIM/ref/book_03865_chp_0016_reader...   \n",
       "7970   NISQA_TRAIN_SIM/ref/book_01454_chp_0004_reader...   \n",
       "4637   NISQA_TRAIN_SIM/ref/book_07907_chp_0003_reader...   \n",
       "6489   NISQA_TRAIN_SIM/ref/book_02922_chp_0034_reader...   \n",
       "4631   NISQA_TRAIN_SIM/ref/book_03191_chp_0009_reader...   \n",
       "\n",
       "                                                    path  \n",
       "11670  /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "3684   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "5187   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "12885  /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "9508   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "...                                                  ...  \n",
       "6433   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "7970   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "4637   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "6489   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "4631   /work/data/speech_metrics_eval/NISQA_Corpus/NI...  \n",
       "\n",
       "[14432 rows x 22 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"mos\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6858/2921841177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.from_numpy(audio_test).float().to(device).unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "# torch.from_numpy(audio_test).float().to(device).unsqueeze(0)\n",
    "t = torch.stack([torch.from_numpy(audio_test).float().to(device),torch.from_numpy(audio_test).float().to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_audio,sr = torchaudio.load(df.sort_values(\"mos\").iloc[0].path)\n",
    "noisy_audio,sr = torchaudio.load(df.sort_values(\"mos\").iloc[-1].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.stack([noisy_audio[:,:10000],noisy_audio[:,:10000]]).to(device),\n",
    "torch.stack([clean_audio[:,:10000],clean_audio[:,:10000]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noisy_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11106/2039743769.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m out = model(torch.stack([noisy_audio[:,:10000],noisy_audio[:,:10000]]).to(device),\n\u001b[0m\u001b[1;32m      2\u001b[0m torch.stack([clean_audio[:,:10000],clean_audio[:,:10000]]).to(device))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noisy_audio' is not defined"
     ]
    }
   ],
   "source": [
    "out = model(torch.stack([noisy_audio[:,:10000],noisy_audio[:,:10000]]).to(device),\n",
    "torch.stack([clean_audio[:,:10000],clean_audio[:,:10000]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11106/733800577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "tp = torch.tensor([int(df.sort_values(\"mos\").iloc[0].mos < df.sort_values(\"mos\").iloc[-1].mos), int(df.sort_values(\"mos\").iloc[0].mos > df.sort_values(\"mos\").iloc[-1].mos)])\n",
    "tp = torch.stack([tp,tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0673, 2.0540], device='cuda:1', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.pow(out[1].cpu(),iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4098, -2.3022], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.pow(tp,iter) * torch.log(torch.pow(out[1].cpu(),iter)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.pow(out[1].cpu(),iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.0427, -6.1038], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log(torch.pow(out[1].cpu(),iter)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = torch.tensor([1.,2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(-tp,iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 2,steps=2)#.to(concat.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_loss(output_preference, true_preference):\n",
    "    iter = torch.tensor([1.,2.]).to(output_preference.device)\n",
    "    return torch.sum(torch.pow(true_preference,iter) * torch.log(torch.pow(output_preference,iter)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_rating_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/data/speech_metrics_eval/NISQA_Corpus/NISQA_corpus_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "torchaudio.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n",
      "48000\n"
     ]
    }
   ],
   "source": [
    "#import librosa\n",
    "import torchaudio\n",
    "for db_type in df[\"db\"].unique():\n",
    "    a,sr = torchaudio.load(df[df[\"db\"] == db_type].iloc[0].path)\n",
    "    print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/work/data/speech_metrics_eval/NISQA_Corpus\"\n",
    "df['path'] = df.apply(lambda x: f\"{dir_path}/{x['db']}/deg/{x['filename_deg']}\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, entries):\n",
    "        self.entries = entries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries.iloc[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'momentum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1976/3206153497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'momentum'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bacace66927486d9dce3112ba76c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f1444ac2b94fc885ff9d3b9d3ffbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1471cb135b4790b18307e99b607634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55baa9884cd64683bb7e1d19090542ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb18580842f848bcb4966f9995daac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ed62c4f2634044aeee0f3d462d85ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/360M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d52d2489de4d46af6fc8b1b00ca0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr_dummy/clean to /home/filip/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8e1a5bb6654d96b8a4ccbba65a0aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a996ffa57d849b2977f56ddb448977a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b5a6581ea640738d1031e39e4969e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34476bc03b2940fe897b66e2512bb2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset librispeech_asr_dummy downloaded and prepared to /home/filip/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# load model and tokenizer\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    \n",
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A MAN SAID TO THE UNIVERSE SIR I EXIST']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['db', 'con', 'file', 'con_description', 'filename_deg', 'filename_ref',\n",
       "       'source', 'lang', 'votes', 'mos', 'noi', 'col', 'dis', 'loud',\n",
       "       'noi_std', 'col_std', 'dis_std', 'loud_std', 'mos_std', 'filepath_deg',\n",
       "       'filepath_ref', 'path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>MOS</th>\n",
       "      <th>std</th>\n",
       "      <th>95%CI</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25765194_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.875</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25634417_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>0.370405</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25705754_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.250</td>\n",
       "      <td>0.886405</td>\n",
       "      <td>0.614248</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25706727_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.523832</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25800071_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.875</td>\n",
       "      <td>0.834523</td>\n",
       "      <td>0.578295</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58704</th>\n",
       "      <td>30083997_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.619806</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58705</th>\n",
       "      <td>29542672_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.800</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58706</th>\n",
       "      <td>30464540_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>2.800</td>\n",
       "      <td>1.095445</td>\n",
       "      <td>0.960200</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58707</th>\n",
       "      <td>28950987_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.800</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>0.733365</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58708</th>\n",
       "      <td>28354886_MinPolqaSfbRecording_cut.wav</td>\n",
       "      <td>3.200</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>0.733365</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58709 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename    MOS       std     95%CI  votes\n",
       "0      25765194_MinPolqaSfbRecording_cut.wav  3.875  0.353553  0.245000      8\n",
       "1      25634417_MinPolqaSfbRecording_cut.wav  1.500  0.534522  0.370405      8\n",
       "2      25705754_MinPolqaSfbRecording_cut.wav  3.250  0.886405  0.614248      8\n",
       "3      25706727_MinPolqaSfbRecording_cut.wav  3.000  0.755929  0.523832      8\n",
       "4      25800071_MinPolqaSfbRecording_cut.wav  3.875  0.834523  0.578295      8\n",
       "...                                      ...    ...       ...       ...    ...\n",
       "58704  30083997_MinPolqaSfbRecording_cut.wav  4.000  0.707107  0.619806      5\n",
       "58705  29542672_MinPolqaSfbRecording_cut.wav  3.800  0.447214  0.392000      5\n",
       "58706  30464540_MinPolqaSfbRecording_cut.wav  2.800  1.095445  0.960200      5\n",
       "58707  28950987_MinPolqaSfbRecording_cut.wav  3.800  0.836660  0.733365      5\n",
       "58708  28354886_MinPolqaSfbRecording_cut.wav  3.200  0.836660  0.733365      5\n",
       "\n",
       "[58709 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"/work/data/speech_metrics_eval/pstn_train/pstn_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f29d6d60ac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.main_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORESQA(\n",
       "  (main_model): MosPredictor(\n",
       "    (ssl_model): Wav2Vec2Model(\n",
       "      (feature_extractor): ConvFeatureExtractionModel(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            (3): GELU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "      (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "      (quantizer): None\n",
       "      (project_q): None\n",
       "      (encoder): TransformerEncoder(\n",
       "        (pos_conv): Sequential(\n",
       "          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (1): SamePad()\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_proj): None\n",
       "    )\n",
       "  )\n",
       "  (linear_layer): Linear(in_features=768, out_features=32, bias=True)\n",
       "  (quantification): PoolAtt(\n",
       "    (linear1): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (linear2): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       "  (preference): PoolAtt(\n",
       "    (linear1): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (linear2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nisqa = pd.read_csv(\"/work/data/speech_metrics_eval/NISQA_Corpus/NISQA_VAL_SIM/NISQA_VAL_SIM_file_pesq_si_sdr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'db', 'con', 'file', 'con_description', 'filename_deg',\n",
       "       'filename_ref', 'source', 'lang', 'votes', 'mos', 'noi', 'col', 'dis',\n",
       "       'loud', 'noi_std', 'col_std', 'dis_std', 'loud_std', 'mos_std',\n",
       "       'filepath_deg', 'filepath_ref', 'filter', 'timeclipping', 'wbgn',\n",
       "       'p50mnru', 'bgn', 'clipping', 'arb_filter', 'asl_in', 'asl_out',\n",
       "       'codec1', 'codec2', 'codec3', 'plcMode1', 'plcMode2', 'plcMode3',\n",
       "       'wbgn_snr', 'bgn_snr', 'tc_fer', 'tc_nburst', 'cl_th', 'bp_low',\n",
       "       'bp_high', 'p50_q', 'bMode1', 'bMode2', 'bMode3', 'FER1', 'FER2',\n",
       "       'FER3', 'asl_in_level', 'asl_out_level', 'si_sdr', 'pesq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nisqa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.000000    168\n",
       "4.000000    135\n",
       "4.200000    132\n",
       "2.000000    123\n",
       "3.600000    122\n",
       "           ... \n",
       "4.714286      1\n",
       "3.300000      1\n",
       "4.900000      1\n",
       "3.777778      1\n",
       "3.900000      1\n",
       "Name: mos, Length: 103, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nisqa[\"mos\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 103 artists>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    " \n",
    "# Figure Size\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# Horizontal Bar Plot\n",
    "plt.bar(nisqa[\"mos\"].unique(), nisqa[\"mos\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.020658016204834"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nisqa[\"pesq\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1976/2533826934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Horizontal Bar Plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnisqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pesq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnisqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pesq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2385\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m         data=None, **kwargs):\n\u001b[0;32m-> 2387\u001b[0;31m     return gca().bar(\n\u001b[0m\u001b[1;32m   2388\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2389\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2340\u001b[0m                 \u001b[0myerr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_dx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2342\u001b[0;31m         x, height, width, y, linewidth, hatch = np.broadcast_arrays(\n\u001b[0m\u001b[1;32m   2343\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m             np.atleast_1d(x), height, width, y, linewidth, hatch)\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(subok, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/miniconda3/envs/sayso_dev/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranges = [0,1,2,3,4,5]\n",
    "df.groupby(pd.cut(df.a, ranges)).count()\n",
    "# Figure Size\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# Horizontal Bar Plot\n",
    "plt.bar(nisqa[\"pesq\"].unique(), nisqa[\"pesq\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43eb8f28c749eccd0c3ad7538053c89983745c874e5d91c778c1295a53c7d146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
